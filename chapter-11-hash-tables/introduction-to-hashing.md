# Introduction to Hashing

**hash table** 是一種可以快速搜尋和新增資料的資料結構，不論儲存多少資料，搜尋和新增的時間複雜度都接近只有 O\(1\)。因為它的操作非常快速，通常運用在需要在很短時間內從上千萬筆資料中找到目標的時候。相較 tree 在操作時的時間複雜度為 O\(logN\)，hash table 不但快而且也比較好實作。

但 hash table 也有一些缺點，因為是基於 array 實作，而 array 一旦建立後便無法輕易增加長度，在某些類型的 hash table 中，但空間快滿時效能會大幅下降，所以必須預測好需要用到的空間大小，或者定時將資料轉移到更大的 hash table 內，但後者也是一個相當耗費時的過程。此外，hash table 很難依排序遍歷儲存的資料。

因此，如果不需要排序遍歷資料同時也可以預估出儲存資料需要的空間大小，那 hash table 在速度和實作方便性上無可匹敵。

hash table 一個重要的概念是如何把 key 值轉換成 array 的 index，hash table 借助 hash function 來完成這項工作。但有些 key 天生就可以直接當成 index 使用，所以我們先以這類 key 做範例，再探討當 key 無法當做 index 時要如何用 hash function。

#### Employee Numbers as Keys

假設今天要做一個員工資料庫，有 1000 名員工資料，每個員工都編了由 1 到 1000 的編號，員工如果離職也不會刪除資料，如果需要快速存取到任一員工的資料時用什麼資料結構最為適當 ?

#### Index Numbers As Keys

一種做法是用最簡單的 array，每個員工資料用編號做為 index 放入 array 中，因為 array 用 index 直接存取資料相當快速，而當有新進員工時要新增資料也很快，因為編號只會遞增所以可以直接將新員工的資料一直加到 array 最末端。

#### Not Always So Orderly

上述的方法看起很不錯，但那是因為 key 是從 1 到已知的最大員工編號，本身即是有規則可循的，而且數量並不多。同時，操作上不會刪除資料，所以不會產生記憶體內浪費的空間，而新的資料可以直接加到 array 末端，並且每次增加上資料不會比現有 array 長度大太多。

#### A Dictionary

許多情況中，key 沒有像員工編號那樣的特性，常見的例子像是字典，如果想要把所有英文字典內的單字都放入記憶體中以快速存取，那 hash table 是個好選擇。

另一個廣泛使用 hash table 的地方是 compiler，它會用 hash table 記錄 1 個 **symbol table** 。symbol table 記錄所有變數、function 名字，和它們在記憶體中儲存的位址。因為程式運作時需要可以快速讀取到相關的變數和 function，所以 hash table 是很適合的資料結構。

#### Converting Words to Numbers

假設今天要儲存 50,000 字的字典，如果用 array 儲存則我們可以用 index 快速存取到單字，但單字和 index 之間的關聯為何，如何定義出每個單字對應的 index ?

電腦使用許多不同的編碼以數字表示文字，比方 ASCII code 是由 0 到 255 以存放字母、標點符號等。我們用一個簡化的方法為例，假設 a 為 1，b 為 2 以此類推，而空格為 0，所以我們有 27 個數字可表示字母。

#### Adding the Digits

一個簡單用數字表示單字的方法是按單字的字母把對應的數字相加，比方 cats：c = 3 a=1 t=20 s= 19，cats = 3 + 1 + 20 + 19 = 43，所以 cats 的 index 就是 43。

假如字典中最長的單字只有 10 個字母，那第 1 個單字 a 會是：

0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 1 = 1

而最後一個單字 zzzzzzzzzz 則是：

26 + 26 + 26 + 26 + 26 + 26 + 26 + 26 + 26 + 26  = 260

因此所有代表單字的數字會從 1 到 260，所以 array 有 260 個元素存放字這些單字，而我們有 50,000 個單字，每個 array 的元素需要存放大約 50,000 / 26 = 192 個單字。

這和目標的 1 個單字對應 1 個 array 元素相差甚遠，即使我們可以在 array 的元素中再用 subarray 或 linked list 存放那 192 個單字，但這樣效率很差。

所以這種方式的問題是會造成許多單字有同樣的 index 因此不夠分散。

#### Multiplying by Powers

如果我們為所有可能的字母組合都建立 1 個對應的數字，這樣就可以確保足夠分散。而要達到這個目的，我們必須要能將單字中每個字母都用唯一的方式表示。

在 10 進位的數字中，每位數的位置都以 10 的次方數表示，比方 7,546 為 7 \* 1000 + 5 \* 100 + 4 \* 10 + 6 \* 1，用次方表示的話即為 7 \* \(10 ^ 3\) + 5 \* \(10 ^ 2\) + 4 \* \(10 ^ 1\) + 6 \* \(10 ^ 0\)，我們將每個位數的單獨數字乘上 10 的位置對應次方再加總就得到代表的數。

相同地，我們可以將單字中每個字母代表的數字乘上 27 \(因為例子中只有 1 到 27 代表字母\) 的位置對應次方數得出該單字的獨特數字，以 cats 為例，就可以轉換成 3 \* \(27 ^ 3\) + 1 \* \(27 ^ 2\) + 20 \* \(27 ^ 1\) + 19 \* \(27 ^ 0\) = 3 \* 19,683 + 1 \* 729 + 20 \* 27 + 19 \* 1 = 59,049 + 729 + 540 + 19 = 60,337。

這種方法可以為每個可能的單字產生唯一的對應數字，但如果字母數一多比方有 10 個字母的 zzzzzzzzzz 會產生 27 ^ 9 以上的數字，記憶體不可能存放得下這麼多的元素。

問題在於這種方法為每種可能的字母組合都產生對應的數字，即使實際上不存在的單字也是，比方 aaaaaaa 這類的字母組合，所以其實多數的數字都是多餘的。

#### Hashing

我們需要一種方法能將前述產生的大量數字壓縮到一個 array 的可接受的元素數量。以 50,000 字的字典為例，這個 array 需要 2 倍的大小，所以是含有 100,000 個元素的 array，因此我們的目標是可以把從 0 到 27 ^ 9 以上的數字壓縮 0 到 100,000 的區間中。

一種簡單的做法是用除法找出餘數。比方我們想把 0 到 199 放入 0 到 9 的區間中，目標區間的數字 \(small number\) 為 0 到 9，所以區間內有 10 個數字可以對應 \(small range\)，而我們將要轉換的數字稱為 large number 的話可以用：

```java
smallNumber = largeNumber % smallRange
```

這樣的程式來找出 larget number 對應的 small number，因為餘數一定落到 0 到 9 中間，比方  13 % 10 = 3 而 157 % 10 = 7，所以回到字典的例子的話：

```text
array index = 單字數字 % array size
```

這樣一個公式就是 **hash** function，它把 1 個大區間內的數字轉換成另 1 個小區間內的數字，而小區間內的數字對應為 array 的 index，用儲存這些 hash 後的資料的 array 就稱為 **hash table** 。

#### Collisions

當我們將代表單字的大區間內的數字轉換成小區間的數字時就無法保證不同的單字不會 hash 成相同的數字，如果一開始使用將字母的數字直接相加的做法一樣。但比直接相加好的是，當直接相加時，我們只有 260 種結果，而用 hash 的方法則可以將結果分散到 50,000 個數字中。

我們的目標是 1 個資料對應到 1 個 array index，但這是不可能的，我們只能儘可能降低對應到同個 array index 的資料數。

設想現在想存入 _melioration_ 這個字到 array 中，將它 hash 後得到 index，但此時發現該格已經存了另一個字 _demystify_ ，因為它 hash 後的 index 是一樣的，這種情況就稱為 **collision** 。

有幾種解決 collision 的方法。一開始我們的 array 大小是實際要放入資料的 2 倍大，因此有一半的空間其實是沒有存放資料的，當發生 collision 時，我們可以用系統性的方式找到閒置空間來存放資料，這種方法稱 **open addressing** ，比方 _cats_ hash 成 5,421，但已經存入另一個字了，我們就改存入 5,422。

另一個方法就是建立 1 個 linked list 存放 collision 的資料，index 內存放則是這個 linked list 的 reference，這種方法稱為 **separate chaining** 。

